{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## Problem Definition\n",
    "For two sets of data points $S_{1}$ and $S_{-1}$, with labels 1 and -1 respectively, scattered in an N-dimensional space, we wish to find a linear hyperplane of dimension N-1 seperating the two sets from each other. This comes under the perview of classification tasks in machine learning. So essentially, we want to classify data points belonging to the two sets. We want to not only do this for points that currently exists in these two sets, but we also want to classify new points into one of these two sets, based on some pattern we picked up from the pre-exising points.So this is also a pattern recognition task.\n",
    "\n",
    "## Intuitive Idea\n",
    "There are three possible scenarios that may play out for the above problem, depending upon the data points that form $S_{1}$ and $S_{-1}$.\n",
    "* There is exactly one such hyperplane that seperates the two sets.\n",
    "* There are many such hyperplanes that seperate the two sets.\n",
    "* There are no hyperplanes that can seperate the two sets.\n",
    "\n",
    "The first scenario is statistically unlikely. So we will consider the other two.\n",
    "\n",
    "As for the second scenario, where multiple such hyperplane exists (and assuming for now that we have some method for finding all of them), we are faced with the problem of choosing the hyperplane which \"best\" solves our problem of classification. Choosing the \"best\" requires us to define some measure using which we can compare two solutions. In the case of SVM this measure is the **width of margins**. \n",
    "\n",
    "Let us try to understand what margins are. Consider any one solution to the classification problem. This solution will be an N-1 dimensional hyperplane, $P$, with all points in $S_{1}$ on one side of it and all points of $S_{-1}$ on the other side. Now take two hyperplanes parallel to $P$, $P_{1}$ and  $P_{-1}$ (called margins) which are on the sides of sets $S_{1}$ and $S_{-1}$ respectively, and keep pushing it out until either one of them touches a data point. Now the perpendicular distance between the planes $P_{1}$ and $P_{-1}$ is called the width of margins of the solution $P$. Once we perform the above actions on all possible solutions we can compare their margin widths and select the one with the largest width as the best solution. This method of classification is called the **Hard Margin SVM**.\n",
    "\n",
    "The reason behind choosing such a measure is simple. The more space there is between the solution and the data points, the more it will **generalize**, which means for example if a new point somewhere close to a point in $S_{1}$ is presented to the classifier, it more likely decide that point belongs to $S_{1}$. \n",
    "\n",
    "Now coming to the third scenario, where the points in the two sets are inseperable using a linear hyperplane, the SVM algorithm affords us two flexible options, namely :\n",
    "* For inseperability caused due to a few points from one set, called outliers, residing in the vicinity of points from the other set we can disregard them, or allow for such small amounts of anomalies. This idea is realized in **Soft Margin SVM**.\n",
    "* For more severe inseperabilities where the sets are completely entangled (but seem to have a pattern to them), we use **kernels** to transform the points into a different space, where they can be linearly seperated (or dealt with using soft margin SVMs).\n",
    "\n",
    "## Formalizing The Idea\n",
    "### Hard Margin SVM\n",
    "Let the solution hyperplane $P$ be characterized by the normal vector to that plane, $w$. This means, the hyperplane is defined as the set of points $x$ such that $w^{T}.x+b=0$. Since we can choose $w$ to have any magnitude as long as it's direction is normal to the hyperplane, without loss of generality we can say that the two other margin hyperplanes, $P_{1}$ and $P_{-1}$, parallel to $P$ can be defined as the set of all points $x$ that satisfy $w^{T}.x+b=1$ and $w^{T}.x+b=-1$ respectively.\n",
    "In order to calculate the width of margins, all we need to do now is take a point from each of the hyperplane margins, and then find it's component in the direction of $w$. Let $x_{1} \\in P_{1}$ and $x_{-1} \\in P_{-1}$. Then the width of margins for the solution $P$ is,\n",
    "\n",
    "$$width=|\\hat{w}^{T}.(x_{1} - x_{-1})| \\\\ $$ \n",
    "$$=||w||^{-1}\\times|w^{T}.x_{1} - w^{T}.x_{-1}|\\\\ $$ \n",
    "$$=||w||^{-1}\\times|w^{T}.x_{1} + b - w^{T}.x_{-1} - b|\\\\ $$ \n",
    "$$=||w||^{-1}\\times|w^{T}.x_{1} + b - (w^{T}.x_{-1} + b)|\\\\ $$ \n",
    "$$=||w||^{-1}\\times|1 - (-1)|\\\\ $$ \n",
    "$$=\\frac{2}{||w||} $$ \n",
    "\n",
    "This is amazing. What we have managed to do is condense our idea of width of margins into a simple formula. Now we need to find a hyperplane which maximizes this quantity. As we have seen above, the solution hyperplane is completely described by $w$ and $b$. So it is by varying these quantities that we will find the desired hyperplane. This fits into the mould of an optimization problem, which can be written down as,\n",
    "\n",
    "$$\\underset{w,b}{maximize}\\  \\frac{2}{||w||}=\\underset{w,b}{minimize}\\  ||w||=\\underset{w,b}{minimize}\\  w^{T}.w$$\n",
    "\n",
    "But this by itself is not sufficient, because no where in this statement are we saying that while the width is maximized, the hyperplane should indeed seperate the points according to the set they belong to. In order to do that, let us introduce a label, $y_i$ for each point $x_i$ such that $y_i=1$ if $x_i \\in S_{1}$ and $y_i=-1$ if $x_i \\in S_{-1}$. This addition allows us to specify the condition that the margins should seperate the sets as,\n",
    "\n",
    "$$y_i\\times(w^{T}.x_i+b)\\geq1$$\n",
    "\n",
    "Now we can completely specify our optimization problem as,\n",
    "\n",
    "$$\\underset{w,b}{minimize}\\  (w^{T}.w)\\ \\ \\ s.t\\ \\ \\ \\ y_i\\times(w^{T}.x_i+b)\\geq1$$\n",
    "\n",
    "In order to solve this, we use the **Largrangian multiplier** method to convert this problem into,\n",
    "\n",
    "$$\\underset{w,b}{minimize}\\ \\ L(w,b,\\alpha)=\\frac{1}{2}w^{T}.w-\\sum_{i}\\alpha_i(y_i(w^{T}x_n+b)-1)$$\n",
    "\n",
    "By differentiating w.r.t $w$ and $b$ and equating to zero we get,\n",
    "\n",
    "$$\\Delta_{w}L=w-\\sum_{i}\\alpha_iy_ix_i=0$$ and\n",
    "\n",
    "$$\\Delta_{b}L=-\\sum_{i}\\alpha_iy_i=0$$\n",
    "\n",
    "Now we go on to substitute for $w$ from $\\Delta_{w}L$, to get\n",
    "\n",
    "$$L(\\alpha)=\\frac{1}{2}(\\sum_{i} \\alpha_{i} y_{i} x_{i})(\\sum_{j} \\alpha_{j} y_{j} x_{j})+\\sum_{i}\\alpha_{i} - b\\sum_{i} \\alpha_{i} y_{i} - (\\sum_{i} \\alpha_{i} y_{i} x_{i})(\\sum_{j} \\alpha_{j} y_{j} x_{j})$$\n",
    "\n",
    "The term with $b$ gets cancelled due to the results obtained from $\\Delta_{b}L$. What remains is,\n",
    "\n",
    "$$L(\\alpha)=\\sum_{i}\\alpha_{i}-\\frac{1}{2}\\sum_{i}\\sum_{j}\\alpha_{i} \\alpha_{j} y_{i} y_{j}x_{i}^{T}x_{j}$$\n",
    "\n",
    "Now we need to maximize this w.r.t to $\\alpha$ (or minimize the negative of the function). This is called the **dual formulation** of our original optimization problem. We have essentially taken a complex problem of mulitple variables and constraints and converted it into a simpler problem which fits into an already known template of optimization problems called **quadratic programming**. Let us state the dual problem fully.\n",
    "\n",
    "$$\\underset{\\alpha}{minimize}\\ \\ -L(\\alpha) \\ \\ \\ s.t\\ \\ \\ \\alpha_{i} \\geq 0\\ \\ \\&\\ \\ \\sum_{i} \\alpha_{i} y_{i} = 0$$\n",
    "\n",
    "The first constraint is inherent to Lagrangian multiplier, i.e they are always assumed to be positive. The second constraint comes form $\\Delta_{b}L$, and should be specified because it places restricition on the values $\\alpha$ can take. The condition laid down by $\\Delta_{w}L$ need not be specified because $w$ is not present in the dual problem.\n",
    "\n",
    "This problem can now be passed into any quadratic optimization solver to get the optimial values of $\\alpha$. Once that is obtained, we can substitute it into the condition given by $\\Delta_{w}L$ to obtain the optimum $w$. \n",
    "\n",
    "$$w=\\sum_{i}\\alpha_{i} y_{i} x_{i}$$\n",
    "\n",
    "Then any one of the given data points can be used to calculate the optimum $b$. For example, if we choose a data point from $S_{1}$ such that $y_i=1$,\n",
    "\n",
    "$$b=1-w^{T}.x_i$$\n",
    "\n",
    "Now given a new point $$x_{new}$$ we can predict which set it belongs to as follows,\n",
    "\n",
    "$$y_{new}=sign(w^{T}.x_{new}+b)$$\n",
    "\n",
    "That is all there is to hard-margin SVM. Simple isn't it.\n",
    "\n",
    "### Transforming Data Space & The Kernel Trick\n",
    "\n",
    "The simplest way to try and seperate two inseperable sets is by simply transforming the vector space they reside in. For example, convert $x=[x_1, x_2, x_3]$ into $x'=[x_1, x_2, x_3, x_1^2, x_2^2, x_3^2]$. If we transform all the points like this such that the sets now become seperable, and then perform the optimization steps discussed above we are done. All that remains is to do this very transformation to any new points before trying to predict which set it belongs to using this classifier.\n",
    "\n",
    "But this transformation can be unnecessary. Moreover, if we want to drastically increase the number of dimensions our data points reside in, it gets computationally heavy as well. Usually this would be a necessary evil. But when we closely look at the SVM formulation, we see that it is not the actual vectors that matter but the dot product between them. The dual problem does not have individual $x_i$ in it but only $x_{i}^{T}.x_{j}$. Similarly, the prediction formula can be re-written as,\n",
    "\n",
    "$$y_{new}=sign(\\sum_{i} \\alpha_{i} y_{i} x_{i}^{T}.x_{new})$$\n",
    "\n",
    "which again shows that all we need is a dot product. How does this help us?\n",
    "Since the dot product is always a single number, how ever large the vectors involved may be (even if they are infinitely long!), if we can find formulae that directly give us the dot product from a transformed space given two vectors in the original space, we can save a lot of computation. This idea is known as the **kernel trick**.\n",
    "\n",
    "So let us define a kernel as $K(x_i, x_j)=\\phi(x_i)^{T} .\\phi(x_j)$. Here $\\phi(x)$ defines the vector transformation. Now we can rewrite the dual problem as,\n",
    "\n",
    "$$L(\\alpha)=\\sum_{i}\\alpha_{i}-\\frac{1}{2}\\sum_{i}\\sum_{j}\\alpha_{i} \\alpha_{j} y_{i} y_{j}K(x_i, x_j)$$\n",
    "\n",
    "and the corresponding prediction formula as,\n",
    "\n",
    "$$y_{new}=sign(\\sum_{i} \\alpha_{i} y_{i} K(x_i, x_{new}))$$\n",
    "\n",
    "But this is just change of notation. Where the kernel trick actually comes into play is when we do not need to compute $\\phi(x_i)^{T} .\\phi(x_j)$ to get $K(x_i, x_j)$ but have a simpler formula for it. For example, take the case of the famous Radial Basis Function (RBF) kernel. Here the kernel is defined by the formula,\n",
    "\n",
    "$$K(x_i, x_j)=exp(-\\gamma \\times||x_i  - x_j||^{2})$$\n",
    "\n",
    "As it happens, this formula is much easier to compute than doing the corresponding transformation followed by dot product because the RBF kernel emulates a dot product in an infinite dimensional vector space!! (courtesy : Taylor series)\n",
    "\n",
    "### Forgiving the Outliers\n",
    "\n",
    "But what if the sets are inseperable due to only a few points, which may have been misplaced due to noise or some abnormality? It is generally not a good idea in machine learning to let outliers like these have a large influence on the algorithm. Transforming the data space just for these few points may be an overkill, when allowing some error will give us a much simpler and interpretable solution. This is where soft-margin SVMs come in.\n",
    "\n",
    "We know that in the seperable case all the data points satisfy the condition,\n",
    "\n",
    "$$y_{i} \\times (w^{T}.x_{i} + b) \\geq 1$$\n",
    "\n",
    "Inseperable cases occur when some points violate this condition, i.e, they end up on the wrong side of the solution hyperplane. In order to allow some error, we define the new condition as,\n",
    "\n",
    "$$y_{i} \\times (w^{T}.x_{i} + b) \\geq 1 - \\zeta_{i}$$\n",
    "\n",
    "where $\\zeta_i \\geq 0$ always,\n",
    "\n",
    "But we cannot allow this error to be very large either, or else our classifier will not be of much use. So we have to minimize the total error $\\sum_{i}\\zeta_{i}$.\n",
    "\n",
    "Now our optimization problem reads,\n",
    "\n",
    "$$\\underset{w,b}{minimize}\\ \\ \\frac{1}{2}w^{T}.w+C \\times \\sum_{i}\\zeta_{i}$$\n",
    "\n",
    "along with the constraints,\n",
    "\n",
    "$$y_{i} \\times (w^{T}.x_{i} + b) \\geq 1 - \\zeta_{i}\\ \\ \\ \\&\\ \\ \\ \\zeta_{i} \\geq 0$$\n",
    "\n",
    "This can be converted into the following Lagrangian form,\n",
    "\n",
    "$$L(w,b,\\alpha,\\beta)=\\frac{1}{2}w^{T}.w+C \\times \\sum_{i}\\zeta_{i}-\\sum_{i}\\alpha_i(y_i(w^{T}x_n+b)-1+\\zeta_i)-\\sum_{i}\\beta_{i}\\zeta_{i}$$\n",
    "\n",
    "where are you can see we now have two sets of Lagrangian multipliers for the two constraints.\n",
    "\n",
    "Now upon minimizing this Lagrangian w.r.t $w$, $b$ and $\\zeta$ we get,\n",
    "\n",
    "$$\\Delta_{w}L=w-\\sum_{i} \\alpha_{i} y_{i} x_{i} = \\vec{0}$$ ,\n",
    "\n",
    "$$\\Delta_{b}L=-\\sum_{i}\\alpha_iy_i=0$$ ,\n",
    "\n",
    "$$\\Delta_{\\zeta}L=C\\times\\vec{1} - \\alpha - \\beta \\times \\vec{1}=0$$ \n",
    " \n",
    "Upon back substituion just as we did above in the hard margin SVM, but by also substituting for $\\beta$ using the formula given by $\\Delta_{\\zeta}L$, we get the dual formulation,\n",
    "\n",
    "$$\\underset{\\alpha}{minimize}\\ \\ -L(\\alpha) \\ \\ \\ s.t\\ \\ \\ \\ C \\geq \\alpha_{i} \\geq 0\\ \\ \\&\\ \\ \\sum_{i} \\alpha_{i} y_{i} = 0$$\n",
    "\n",
    "As you can see, the only difference in the dual problem in the soft-margin case is that $\\alpha$ is now also has an upper bound $C$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
