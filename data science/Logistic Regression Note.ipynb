{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression : A Generalization of Linear Regression\n",
    "\n",
    "## Introduction\n",
    "Linear regression is a commonly used tool in statistics. The linear regression model assumes a linear relationship between the input variables and the output variable. That means we assume the output to be a weighted sum of the inputs. Mathematically this is written as,\n",
    "\n",
    "$$y=w^{T} . x = L(x,w)$$\n",
    "\n",
    "where $y$ is the output predicted by the linear regression model, $x$ is the vector consisting of the input variable values, $W$ is the weight associated with each input and $w$ are the numerical values for those weights. $L(x,w)$ represents our linear model.\n",
    "\n",
    "This model gives you a real numbered output between $-\\infty$ to $\\infty$. This is good when we require a continuous valued output. But sometimes what we need is an output that tells us to which class a particular input vector belongs. Or even better, the probability with which it belongs to a particular class.\n",
    "\n",
    "Since linear regression is such a common model, naturally there has been attempts to tweak this model so as to achive this requirement, and logistic regression is the most popular outcome of this effort. \n",
    "\n",
    "## The Range Mismatch \n",
    "So now we need to generalize linear regression to output a probability instead of some real number. Ideally, if we want to model something as a linear regression all we need to do is equate it to $L(x,w)$. So in this case since we want to model the probability of an input $x$ being in say, class 1, we should ideally do something like,\n",
    "\n",
    "$$P(x,w) = L(x, W=w) = w^{T}\\cdot x$$\n",
    "\n",
    "where $P(x,w) = Prob(class=1 | x; w)$ is the probability that the input $x$ belongs to class 1. But this won't hold up mathematically unless you place constraints on $w$ and $x$ (and it is not a good idea to palce constraints on the inputs). This is because the weighted sum can take up any value between $-\\infty$ to $\\infty$ but probability should always be a value between $0$ and $1$ (inclusive). So, the range of the function $P(x,w)$ and $L(x,w)$ does not match. How do we solve this problem?\n",
    "\n",
    "## Constructing a function of P(x,w) with Matching Range\n",
    "Since ranges of the two functions do not match we need to change one of them. And since we have already decided to stick to the linear model, we only have one choice, i.e, to change $P(x,w)$. But isn't our aim to predict the probability of an input being in a particular class? So how can we take $P(x,w)$ out of the equation? We can't. But what we can do is find a function of $P(x,w)$ which has a range of $( -\\infty, \\infty )$. Ofcourse, there are probably an infinite number of ways to map the interval $[0,1]$ to $( -\\infty, \\infty )$. We will choose one which is well interpretable (and which leads to the logistic regression model, since thats what this post is all about!). Let us do this in two steps.\n",
    "\n",
    "1. We define a function called \"odds\" function represented as \n",
    "    $$\\mathbb{O}(P(x,w))=\\frac{P(x,w)}{1-P(x,w)}$$\n",
    "\n",
    "    A few quick calculations will show that,\n",
    "\n",
    "    $\\mathbb{O}(0) = 0$\n",
    "\n",
    "    $\\mathbb{O}(0.5) = 1$\n",
    "\n",
    "    $\\mathbb{O}(1) = \\infty$\n",
    "\n",
    "    We can see that this function has a range of $[0, \\infty)$. Also note that this function seems to be unevenly distributed for it's inputs. For inputs from the interval $[0, 0.5]$ it gives outputs from the interval $[0, 1]$ but for inputs from the interval $(0.5, 1]$ it gives outputs from the interval $(1, \\infty)$. This is despite the fact that both these input intevals are of the same size. This function again, cannot be fitted with $L(x,w)$ because of the range mismatch.\n",
    "    \n",
    "2. Now let us perform anothe transformation on $\\mathbb{O}$ such that it's range expands to $(-\\infty, \\infty)$. This will be the function to which we fit $L(x,w)$. Again, there are many possible mappings of a function from $[0, \\infty)$ to $(-\\infty, \\infty)$ but it turns out taking the natural log of $\\mathbb{O}$ gives us the desired range, while also being easy to work with (rather surprisingly!). This function is called the \"log odds\" function. So lets define,\n",
    "\n",
    "    $$\\mathbb{R}(\\mathbb{O}(P(x,w))) = ln(\\mathbb{O}(P(x,w))) = ln \\left( \\frac{P(x,w)}{1-P(x,w)} \\right) = \\mathbb{R}(P(x,w))$$\n",
    "\n",
    "    By definition, \n",
    "    $$b = ln(c) \\implies e^{b} = c$$ \n",
    "    So obviously,\n",
    "\n",
    "    $\\mathbb{R}(P(x,w)=1) = ln(\\infty) =\\infty$\n",
    "\n",
    "    $\\mathbb{R}(P(x,w)=0) = ln(0) =-\\infty$\n",
    "\n",
    "    $\\mathbb{R}(P(x,w)=0.5) = ln(1) =0$\n",
    "\n",
    "    This function is interesting. For example, if the probability of the input $x$ being in class 1 is $P(x,w)=0.7$, then the log odds is $\\mathbb{R}(0.7)=0.8473$. On the contrary, the log odds of the input $x$ not being in class 1 ($Q(x,w)=1-P(x,w)=0.3$) is $\\mathbb{R}(0.3)=-0.8473$. So the sum total of log odds of these only two possible options is $\\mathbb{R}(P(x,w))+\\mathbb{R}(Q(x,w))=0$. This is similar to $P(x,w)+Q(x,w)=1$. So we have managed to construct a function that has a range matching $L(x,w)$ and has properties similar to $P(x,w)$.\n",
    "\n",
    "    Now finally we can write,\n",
    "    $$\\mathbb{R}(P(x,w)) = ln \\left( \\frac{P(x,w)}{1-P(x,w)} \\right) =w^{T}\\cdot x=L(x, W=w)$$\n",
    "\n",
    "## Finding the Probability\n",
    "What we are interested in is the probability $P(x,w)=Prob(class=1 | x; w)$. We can retreive that as follows,\n",
    "\n",
    "$$ln \\left( \\frac{P(x,w)}{1-P(x,w)} \\right) =w^{T}\\cdot x$$ \n",
    "$$\\frac{P(x,w)}{1-P(x,w)} = e^{w^{T}\\cdot x}$$ \n",
    "$$P(x,w)(1 + e^{w^{T}\\cdot x}) = e^{w^{T}\\cdot x}$$ \n",
    "$$P(x,w) = \\frac{e^{w^{T}\\cdot x}}{1 + e^{w^{T}\\cdot x}} = S(x, w)$$\n",
    "\n",
    "We have arrived at the logistic model $S(x,w)$ which you may have seen before in connection with logistic regression. This function is in fact called the \"sigmoid\" function. This name is due to the fact that the graph of this function is S-shaped.\n",
    "\n",
    "## The Derivative\n",
    "One amazing thing about the sigmoid function is that it's derivative looks very neat. Lets derive it.\n",
    "\n",
    "$$\\frac{\\delta S(x,w)}{\\delta w} = \\frac{\\delta}{\\delta w}\\left( \\frac{e^{w^{T}\\cdot x}}{1 + e^{w^{T}\\cdot x}} \\right)$$ \n",
    "\n",
    "$$=\\left( \\frac{e^{w^{T}\\cdot x}}{(1 + e^{w^{T}\\cdot x})^{2}} \\right) \\cdot x$$ \n",
    "\n",
    "$$= \\left( \\frac{e^{w^{T}\\cdot x}}{1 + e^{w^{T}\\cdot x}} \\times  \\frac{1}{1 + e^{w^{T}\\cdot x}} \\right) \\cdot x$$ \n",
    "\n",
    "$$\\frac{\\delta S(x,w)}{\\delta w} = S(x,w) \\times (1 - S(x,w)) \\cdot x$$ \n",
    "\n",
    "This can be used when we want to compute $w$ using iterative algorithms like gradient descent.\n",
    "\n",
    "## Computation\n",
    "### Cost Function\n",
    "The cost function that is usually minimized to find $w$ when using the logistic model is cross-entropy. This function takes the form,\n",
    "\n",
    "$$C(S(x,w),y) = - (y \\times ln(S(x,w)) + (1 - y) \\times ln(1 - S(x,w)))$$\n",
    "\n",
    "where $y$ is the actual output (assumed to be 1 if the input is in class 1, 0 otherwise) and $S(x,w)$ is the probability of input being in class 1 as calculated by the logistic model. \n",
    "\n",
    "Let's see what this cross-entropy function is telling us. Imagine the actual output is $y=1$. In that case, the function becomes,\n",
    "\n",
    "$$C(S(x,w),y) = - 1 \\times ln(S(x,w))$$\n",
    "\n",
    "Now if $S(x,w)$ is close to 1 $ln(S(x,w))$ will tend to 0, and $C(S(x,w),y)$ will also tend to 0. But if $S(x,w)$ is closer to 0 (i.e the logistic model's output contradicts the actual output), then $ln(S(x,w))$ will tend to $-\\infty$ which will make $C(S(x,w),y)$ tend to $\\infty$. Now you know why that negative sign is important in the cross-entropy function's equation.\n",
    "\n",
    "Similarly, when $y=0$, the function simpilifies to,\n",
    "\n",
    "$$C(S(x,w),y) = -1 \\times ln(1 - S(x,w))$$\n",
    "\n",
    "Now when $S(x,w)$ is close to 1, $ln(1-S(x,w))$ tends to $-\\infty$ and when $S(x,w)$ is closer to 0, $ln(1- S(x,w))$ tends to 0.\n",
    "\n",
    "We can see that the cross-entropy function just takes on a bigger value when the actual output and the predicted output are mismatched. So it is a good fit for the role of a cost function.\n",
    "\n",
    "### Gradient Descent\n",
    "The gradient descent algorithm can be summarized as,\n",
    "\n",
    "$$w_{new} = w_{old} - \\alpha \\times \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\frac{\\delta C(S(x_{i},w),y_{i})}{\\delta w} \\right]_{w=w_{old}}$$\n",
    "\n",
    "where $\\alpha$ is a tunable parameter called \"learning rate\". \n",
    "\n",
    "The gradient descent algorithms will :\n",
    "1. Start with some random value for $w$.\n",
    "\n",
    "2. Compute S(x,w) with this $w$.\n",
    "\n",
    "3. Compute the cost function $C(S(x,w), y)$ for different samples (i.e x,y pairs).\n",
    "\n",
    "4. Compute the derivative $\\left[ \\frac{\\delta C(S(x,w),y)}{\\delta w} \\right]_{w=w_{old}}$ for each sample.\n",
    "\n",
    "5. Average out the value for the derivative across all samples.\n",
    "\n",
    "6. Apply the above formula to find the new $w$.\n",
    "\n",
    "7. Repeat until some stoppage criteria is met.\n",
    "\n",
    "The important step here is to compute the derivative of the cost function w.r.t $w$. Let's derive this.\n",
    "\n",
    "Using chain rule for derivatives,\n",
    "\n",
    "$$\\frac{\\delta C(S(x),y)}{\\delta w} = \\frac{\\delta C(S(x,w), y)}{\\delta S(x,w)} \\times \\frac{\\delta S(x,w)}{w}$$\n",
    "\n",
    "Using multiplication rule for derivatives for the first derivative and using the derivative for $S(x,w)$ we derived in the previous section for the second derivative we get,\n",
    "\n",
    "$$\\frac{\\delta C(S(x),y)}{\\delta w}  = - \\left( \\frac{y}{S(x,w)} - \\frac{1-y}{1-S(x,w)} \\right) \\times S(x,w) \\times (1 - S(x,w)) \\cdot x = \\frac{(S(x,w) - y) \\times S(x,w) \\times (1 - S(x,w))}{S(x,w) \\times (1 - S(x,w))} \\cdot x$$\n",
    "\n",
    "Cancelling out the common terms in the numerator and denominator we finally get,\n",
    "\n",
    "$$\\frac{\\delta C(S(x),y)}{\\delta w}  = (S(x,w) - y) \\cdot x$$\n",
    "\n",
    "### Including a Bias\n",
    "Normally, the linear model that is being fit will also include a weight variable which is added without being multiplied to any input. This makes the linear model look like,\n",
    "\n",
    "$$L(x,w,b)=w^{T} \\cdot x + b$$\n",
    "\n",
    "I am talking about the extra $b$ variable. We can easily add a bias term to the linear model without any change of notation if we do the following,\n",
    "1. Append a 1 to the input vector. i.e, let $x = [1, x]^{T}$\n",
    "2. Append the variable $b$ to the weight vector. i.e, let $w = [b, w]^{T}$\n",
    "\n",
    "By modifying the variables in this manner, you can account for the bias term without any change in the notation discussed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding a Logistic Regression Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    z = np.exp(-np.matmul(x, w))\n",
    "    return np.ones_like(z) / (np.ones_like(z) + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(s, y):\n",
    "    epsilon = 1e-7\n",
    "    zero_part = y * np.log(s + epsilon)\n",
    "    one_part = (np.ones_like(y) - y) * np.log(np.ones_like(s) - s + epsilon)\n",
    "    val = - (1 / y.shape[0]) * np.sum(zero_part + one_part)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(s, y, x):\n",
    "    return (s - y) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, alpha, num_iter=10000):\n",
    "    num_inputs = features.shape[0]\n",
    "    input_size = features.shape[1]\n",
    "    \n",
    "    w = np.random.rand(input_size, 1)\n",
    "    for i in range(num_iter) :\n",
    "        s = sigmoid(features, w)\n",
    "        c = cross_entropy(s, labels)\n",
    "        g = np.sum(gradient(s, labels, features).T, axis=1, keepdims=True) / num_inputs\n",
    "        w = w - alpha * g\n",
    "        \n",
    "        if (i+1) % 1000 == 0 :  \n",
    "            print(\"Iteration  : \", i + 1, \" cost : \", c)\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some dummy data and labels to try out this model. Notice that we are appending an array of ones to the features in order to account for the bias weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([10, 15, 5, 7]).reshape((4, 1))\n",
    "features = np.hstack((features, np.ones((4,1))))\n",
    "labels = np.array([1, 1, 0, 0]).reshape((4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the weights required to correctly predict the class labels, using gradient descent. The bias term will also be included in the weight vector returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  :  1000  cost :  0.033237116194023704\n",
      "Iteration  :  2000  cost :  0.022155567944981008\n",
      "Iteration  :  3000  cost :  0.016604891197097918\n"
     ]
    }
   ],
   "source": [
    "w = train(features, labels, 0.5, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the features, labels and the learned sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(features[:,0], labels, color='r')\n",
    "sig_x = np.linspace(np.min(features[:,0]), np.max(features[:,0]), num=100).reshape((100,1))\n",
    "sig_x = np.hstack((sig_x, np.ones((100,1))))\n",
    "print(sig_x.shape)\n",
    "sig_y = sigmoid(sig_x, w)\n",
    "print(sig_y.shape)\n",
    "plt.plot(sig_x[:,0], sig_y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the weights have been adjusted in such a way that the sigmoid function takes a value near 1 for samples labelled 1, and 0 for sampels labelled 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Weights Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.273]\n",
      " [-19.146]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={\"float_kind\": lambda x: \"%.3f\" % x}) # just something to make numpy arrays print pretty\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Numerical Output from the Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.973]\n",
      " [1.000]\n",
      " [0.000]\n",
      " [0.038]]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(features, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
